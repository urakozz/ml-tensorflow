{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'term',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'against']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "  return data\n",
    "  \n",
    "words = read_data(filename)\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordNumericEncoder:\n",
    "    def __init__(self, words, common=0, rare_word_token=\"UNK\"):\n",
    "        self._words = words\n",
    "        self._rare_word_token = rare_word_token\n",
    "        self._counter = collections.Counter(words)\n",
    "        \n",
    "        self._set_items(common)\n",
    "        self._build_dictionary()\n",
    "        self._encode_words()\n",
    "        \n",
    "    def _set_items(self, common=0):\n",
    "        self._items = [[self._rare_word_token, -1]]\n",
    "        if common <= 0:\n",
    "            common = len(self._words)\n",
    "            self._items = []\n",
    "        self._items.extend(self._counter.most_common(common))\n",
    "    \n",
    "    def _build_dictionary(self):\n",
    "        self._dictionary = dict()\n",
    "        for word, _ in self._items:\n",
    "            self._dictionary[word] = len(self._dictionary)\n",
    "    \n",
    "    def _encode_words(self):\n",
    "        data = list()\n",
    "        unk_count = 0\n",
    "        for word in self._words:\n",
    "            if word in self._dictionary:\n",
    "                index = self._dictionary[word]\n",
    "            else:\n",
    "                index = 0  # items['UNK']\n",
    "                unk_count = unk_count + 1\n",
    "            data.append(index)\n",
    "        self._items[0][1] = unk_count\n",
    "        self._data = data\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self._data\n",
    "    \n",
    "    def get_reverse_dictionary(self):\n",
    "        return dict(zip(self._dictionary.values(), self._dictionary.keys())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ContextBatchGenerator:\n",
    "    \n",
    "    def __init__(self, text, window):\n",
    "        self._text = text # -> [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"]\n",
    "        self._len = len(text)\n",
    "        self._window = window # -> 2\n",
    "        self._cursor = 0\n",
    "        self._span = window*2 + 1 # -> 5 [window... , target, window...]\n",
    "        self._non_window_idx = [i for i in range(self._span) if i != window] # -> [0,1,3,4]\n",
    "        self._buffer = collections.deque(maxlen=self._span)\n",
    "        for i in range(self._span):\n",
    "            self.shift_buffer()\n",
    "        # -> buffer = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "    \n",
    "    def _batch(self, size):\n",
    "        l = list()\n",
    "        for i in range(size):\n",
    "            target = self._buffer[self._window] # -> buffer[2]\n",
    "            context = [self._buffer[i] for i in self._non_window_idx] # buffer ['1','2','4','5']\n",
    "            l.append((context, target)) # ->(['1', '2', '4', '5'], '3')\n",
    "            self.shift_buffer()\n",
    "        return l\n",
    "            \n",
    "    def shift_buffer(self):\n",
    "        self._buffer.append(self._text[self._cursor])\n",
    "        self._cursor = (self._cursor + 1) % self._len\n",
    "\n",
    "class SkipGramGenerator(ContextBatchGenerator):\n",
    "    \n",
    "    def next(self, size, dtype=np.int32):\n",
    "        if (size % (self._window*2) !=0):\n",
    "            raise ValueError(\"batch size should be devidable by window*2\")\n",
    "        batches = size // (self._window*2)\n",
    "        \n",
    "        batch = np.ndarray(shape=(size), dtype=dtype)\n",
    "        labels = np.ndarray(shape=(size, 1), dtype=dtype)\n",
    "        i = 0\n",
    "        for b in self._batch(batches):\n",
    "            for t in b[0]:\n",
    "                batch[i] = t\n",
    "                labels[i] = b[1]\n",
    "                i+=1\n",
    "        return batch, labels # next(2) -> [10, 30], [[20], [20]]\n",
    "    \n",
    "class CBOWGenerator(ContextBatchGenerator):\n",
    "    \n",
    "    def next(self, size, dtype=np.int32):\n",
    "        if (size % (self._window*2) !=0):\n",
    "            raise ValueError(\"batch size should be devidable by window*2\")\n",
    "        batches = size // (self._window*2)\n",
    "        \n",
    "        batch = np.ndarray(shape=(size), dtype=dtype)\n",
    "        labels = np.ndarray(shape=(size, 1), dtype=dtype)\n",
    "        i = 0\n",
    "        for b in self._batch(batches):\n",
    "            for t in b[0]:\n",
    "                batch[i] = b[1] # <--\n",
    "                labels[i] = t # <--\n",
    "                i+=1\n",
    "        return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "window = 2\n",
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "num_sampled = 64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels  = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    \n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    nce_w = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(nce_w, nce_b, embed,\n",
    "                               train_labels, num_sampled, vocabulary_size))\n",
    "    \n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_encoded = WordNumericEncoder(words, vocabulary_size)\n",
    "batch = CBOWGenerator(word_encoded.get_data(), window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reverse = word_encoded.get_reverse_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "validate ['eight', 'where']\n",
      "closest [10 17  4  8 21 16  9 22 23 13] ['two', 'three', 'one', 'zero', 'four', 'five', 'nine', 'six', 'seven', 'eight']\n",
      "closest [31352   643 12330  8410 22340 16401   212 40955  1609   100] ['obituaries', 'jews', 'herb', 'embraced', 'consolidating', 'playwrights', 'include', 'amorites', 'description', 'where']\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = batch.next(batch_size)\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        if step%100 == 0:\n",
    "            print(step)\n",
    "    \n",
    "    valid = [13,100]\n",
    "    print(\"validate\", [reverse[i] for i in valid])\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid)\n",
    "    \n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n",
    "    sim = similarity.eval()\n",
    "    for i in range(len(valid)):\n",
    "        closest = sim[i].argsort()[-10:]\n",
    "        print(\"closest\", closest, [reverse[i] for i in closest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
